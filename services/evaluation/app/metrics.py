"""Heuristic metric functions for answer evaluation.

These functions compute very simple metrics based on substring
containment. They are not robust but are sufficient to demonstrate
the evaluation API. In a real system you would replace these with
proper implementations leveraging natural language inference models
and retrieval precision/recall calculations.
"""

from __future__ import annotations

from typing import List, Tuple

from shared.models import DocChunk


def simple_metric_scores(
    question: str, answer: str, sources: List[DocChunk]
) -> Tuple[float, float, float]:
    """Compute naive factuality, relevance and completeness scores.

    Args:
        question: The user question.
        answer: The answer generated by the system.
        sources: The list of document chunks used to answer.

    Returns:
        A tuple of three floats in the range [0,1].
    """
    # Concatenate source text for containment checks
    combined = " ".join(chunk.text.lower() for chunk in sources)
    ans_lower = answer.lower()
    # Factuality: 1 if every word in answer appears in sources, else 0
    if not ans_lower.strip():
        factuality = 0.0
    else:
        answer_terms = set(ans_lower.split())
        combined_terms = set(combined.split())
        factuality = 1.0 if answer_terms.issubset(combined_terms) else 0.0
    # Relevance: 1 if answer contains any word from sources, else 0
    relevance = 1.0 if any(word in combined for word in ans_lower.split()) else 0.0
    # Completeness: 1 if answer is non-empty, else 0
    completeness = 1.0 if ans_lower.strip() else 0.0
    return factuality, relevance, completeness